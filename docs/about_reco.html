<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>About Recommenders Metrics &mdash; Jurity 1.3.2 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Jurity Public API" href="api.html" />
    <link rel="prev" title="About Algorithmic Fairness" href="about_fairness.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Jurity
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="about_fairness.html">About Algorithmic Fairness</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">About Recommenders Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#binary-recommender-metrics">Binary Recommender Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ctr-click-through-rate">CTR: Click-through Rate</a></li>
<li class="toctree-l3"><a class="reference internal" href="#auc-area-under-the-curve">AUC: Area Under the Curve</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ranking-recommender-metrics">Ranking Recommender Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#precision">Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recall">Recall</a></li>
<li class="toctree-l3"><a class="reference internal" href="#map-mean-average-precision">MAP: Mean Average Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ndcg-normalized-discounted-cumulative-gain">NDCG: Normalized Discounted Cumulative Gain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#diversity-recommender-metrics">Diversity Recommender Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#inter-list-diversity">Inter-List Diversity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#intra-list-diversity">Intra-List Diversity</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Jurity Public API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Jurity</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>About Recommenders Metrics</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/about_reco.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="about-recommenders-metrics">
<span id="about-reco"></span><h1>About Recommenders Metrics<a class="headerlink" href="#about-recommenders-metrics" title="Permalink to this headline"></a></h1>
<p>Jurity offers various standardized metrics for measuring the recommendation performance.
While the recommendation systems community agrees on these metrics, the implementations can be different, especially when it comes to edge-cases.</p>
<p>For the definitions below, <span class="math notranslate nohighlight">\(A\)</span> is the set of actual ratings for users and <span class="math notranslate nohighlight">\(P\)</span> is the set of predictions / recommendations.
Each <span class="math notranslate nohighlight">\(A_i\)</span> and <span class="math notranslate nohighlight">\(P_i\)</span> represents the list of actual items and list of recommended items, respectively, for a user <span class="math notranslate nohighlight">\(i\)</span>.</p>
<section id="binary-recommender-metrics">
<h2>Binary Recommender Metrics<a class="headerlink" href="#binary-recommender-metrics" title="Permalink to this headline"></a></h2>
<p>Binary recommender metrics directly measure the click interaction.</p>
<section id="ctr-click-through-rate">
<h3>CTR: Click-through Rate<a class="headerlink" href="#ctr-click-through-rate" title="Permalink to this headline"></a></h3>
<p>CTR offers three reward estimation methods.</p>
<p>Direct estimation (“matching”) measures the accuracy of the recommendations over the subset of user-item pairs that appear in both actual ratings and recommendations.</p>
<p>Let <span class="math notranslate nohighlight">\(M\)</span> denote the set of user-item pairs that appear in both actual ratings and recommendations, and <span class="math notranslate nohighlight">\(C(M_i)\)</span> be an indicator function that produces <span class="math notranslate nohighlight">\(1\)</span> if the user clicked on the item, and <span class="math notranslate nohighlight">\(0\)</span> if they didn’t.</p>
<div class="math notranslate nohighlight">
\[CTR = \frac{1}{\left | M \right |}\sum_{i=1}^{\left | M \right |} C(M_i)\]</div>
<p>Inverse propensity scoring (IPS) weights the items by how likely they were to be recommended by the historic policy
if the user saw the item in the historic data. Due to the probability inversion, less likely items are given more weight.</p>
<div class="math notranslate nohighlight">
\[IPS = \frac{1}{n} \sum r_a \times \frac{I(\hat{a} = a)}{P(a|x,h)}\]</div>
<p>In this calculation: n is the total size of the test data; <span class="math notranslate nohighlight">\(r_a\)</span> is the observed reward;
<span class="math notranslate nohighlight">\(\hat{a}\)</span> is the recommended item; <span class="math notranslate nohighlight">\(I(\hat{a} = a)\)</span> is a boolean of whether the user-item pair has
historic data; and <span class="math notranslate nohighlight">\(P(a|x,h)\)</span> is the probability of the item being recommended for the test context given
the historic data.</p>
<p>Doubly robust estimation (DR) combines the directly predicted values with a correction based on how
likely an item was to be recommended by the historic policy if the user saw the item in the historic data.</p>
<div class="math notranslate nohighlight">
\[DR = \frac{1}{n} \sum \hat{r}_a + \frac{(r_a -\hat{r}_a) I(\hat{a} = a)}{p(a|x,h)}\]</div>
<p>In this calculation, <span class="math notranslate nohighlight">\(\hat{r}_a\)</span> is the predicted reward.</p>
<p>At a high level, doubly robust estimation combines a direct estimate with an IPS-like correction if historic data is
available. If historic data is not available, the second term is 0 and only the predicted reward is used for the
user-item pair.</p>
<p>The IPS and DR implementations are based on: Dudík, Miroslav, John Langford, and Lihong Li.
“Doubly robust policy evaluation and learning.” Proceedings of the 28th International Conference on International
Conference on Machine Learning. 2011. Available as arXiv preprint arXiv:1103.4601</p>
</section>
<section id="auc-area-under-the-curve">
<h3>AUC: Area Under the Curve<a class="headerlink" href="#auc-area-under-the-curve" title="Permalink to this headline"></a></h3>
<p>AUC is the probability that the recommender will rank a randomly chosen relevant/clicked instance higher than a randomly chosen non-relevant/not-clicked one over the subset of user-item pairs that appear in both actual ratings and recommendations.</p>
<p>Let <span class="math notranslate nohighlight">\(M\)</span> denote the set of user-item pairs that appear in both actual ratings and recommendations with <span class="math notranslate nohighlight">\(M^1\)</span> the set of relevant/clicked instances and <span class="math notranslate nohighlight">\(M^0\)</span> the non-clicked instances.
If <span class="math notranslate nohighlight">\(f(t)\)</span> is the score returned by the recommender for the <span class="math notranslate nohighlight">\(t\)</span>-th instance then:</p>
<div class="math notranslate nohighlight">
\[AUC = \frac{\sum_{t_0 \in M^0}\sum_{t_1 \in M^1}I[(f(t_0) &lt; f(t_1)]}{\left | M^0 \right | \left | M^1 \right |}\]</div>
</section>
</section>
<section id="ranking-recommender-metrics">
<h2>Ranking Recommender Metrics<a class="headerlink" href="#ranking-recommender-metrics" title="Permalink to this headline"></a></h2>
<p>Ranking metrics reward putting the clicked items on a higher position/rank than the others.
Some metrics use the relevance function <span class="math notranslate nohighlight">\(rel(P_{i,n})\)</span>, which is an indicator function that produces <span class="math notranslate nohighlight">\(1\)</span> if the predicted item at position <span class="math notranslate nohighlight">\(n\)</span> for user <span class="math notranslate nohighlight">\(i\)</span> is in the user’s relevant set of items.</p>
<p>All the ranking metrics operate on a filtered set of users such that only the users with relevant/clicked items are taken into account.
This is in line with industry practices.
There is a further filtering for precision related metrics (Precision&#64;k and MAP&#64;k) where each user also has to have a recommendation.
This is done to avoid divide by 0 errors.</p>
<section id="precision">
<h3>Precision<a class="headerlink" href="#precision" title="Permalink to this headline"></a></h3>
<p>Precision&#64;k measures how consistently a model is able to pinpoint the items a user would interact with.
A recommender system that only provides recommendations for 5% of the users will have a high precision if the users receiving the recommendations always interact with them.</p>
<div class="math notranslate nohighlight">
\[Precision&#64;k = \frac{1}{\left | A \cap P \right |}\sum_{i=1}^{\left | A \cap P \right |} \frac{\left | A_i \cap P_i[1:k] \right |}{\left | P_i[1:k] \right |}\]</div>
</section>
<section id="recall">
<h3>Recall<a class="headerlink" href="#recall" title="Permalink to this headline"></a></h3>
<p>Recall&#64;k measures whether a model can capture all the items the user has interacted with.
If __k__ is high enough, a recommender system can get a high recall even if it has a large amount of irrelevant recommendations, if it has also identified the relevant recommendations.</p>
<div class="math notranslate nohighlight">
\[Recall&#64;k = \frac{1}{\left | A \right |}\sum_{i=1}^{\left | A \right |} \frac{\left | A_i \cap P_i[1:k] \right |}{\left | A_i \right |}\]</div>
</section>
<section id="map-mean-average-precision">
<h3>MAP: Mean Average Precision<a class="headerlink" href="#map-mean-average-precision" title="Permalink to this headline"></a></h3>
<p>MAP&#64;k measures a position-sensitive version of Precision&#64;k, where getting the top-most recommendations more precise has a more important effect than getting the last recommendations correct.
When <span class="math notranslate nohighlight">\(k=1\)</span>, Precision&#64;k and MAP&#64;k are the same.</p>
<div class="math notranslate nohighlight">
\[MAP&#64;k = \frac{1}{\left | A \right |} \sum_{i=1}^{\left | A \right |} \frac{1}{min(k,\left | A_i \right |))}\sum_{n=1}^k Precision_i(n) \times rel(P_{i,n})\]</div>
</section>
<section id="ndcg-normalized-discounted-cumulative-gain">
<h3>NDCG: Normalized Discounted Cumulative Gain<a class="headerlink" href="#ndcg-normalized-discounted-cumulative-gain" title="Permalink to this headline"></a></h3>
<p>NDCG&#64;k measures the relevance of the ranked recommendations discounted by the rank at which they appear.
It is normalized to be between 0 and 1.
Improving the highest-ranked recommendations has a more important effect than improving the lowest-ranked recommendations.</p>
<div class="math notranslate nohighlight">
\[NDCG&#64;k = \frac{1}{\left | A \right |} \sum_{i=1}^{\left | A \right |} \frac {\sum_{r=1}^{\left | P_i \right |} \frac{rel(P_{i,r})}{log_2(r+1)}}{\sum_{r=1}^{\left | A_i \right |} \frac{1}{log_2(r+1)}}\]</div>
</section>
</section>
<section id="diversity-recommender-metrics">
<h2>Diversity Recommender Metrics<a class="headerlink" href="#diversity-recommender-metrics" title="Permalink to this headline"></a></h2>
<p>Diversity recommender metrics evaluate the quality of recommendations for different notions of diversity.</p>
<section id="inter-list-diversity">
<h3>Inter-List Diversity<a class="headerlink" href="#inter-list-diversity" title="Permalink to this headline"></a></h3>
<p>Inter-List Diversity&#64;k measures the inter-list diversity of the recommendations when only k recommendations are
made to the user. It measures how user’s lists of recommendations are different from each other. This metric has a range
in <span class="math notranslate nohighlight">\([0, 1]\)</span>. The higher this metric is, the more diversified lists of items are recommended to different users.
Let <span class="math notranslate nohighlight">\(U\)</span> denote the set of <span class="math notranslate nohighlight">\(N\)</span> unique users, <span class="math notranslate nohighlight">\(u_i\)</span>, <span class="math notranslate nohighlight">\(u_j \in U\)</span> denote the i-th and j-th user in the
user set, <span class="math notranslate nohighlight">\(i, j \in \{0,1,\cdots,N\}\)</span>. <span class="math notranslate nohighlight">\(R_{u_i}\)</span> is the binary indicator vector representing provided
recommendations for <span class="math notranslate nohighlight">\(u_i\)</span>. <span class="math notranslate nohighlight">\(I\)</span> is the set of all unique user pairs, <span class="math notranslate nohighlight">\(\forall~i&lt;j, \{u_i, u_j\} \in I\)</span>.</p>
<div class="math notranslate nohighlight">
\[Inter\mbox{-}list~diversity = \frac{\sum_{i,j, \{u_i, u_j\} \in I}(cosine\_distance(R_{u_i}, R_{u_j}))}{|I|}\]</div>
</section>
<section id="intra-list-diversity">
<h3>Intra-List Diversity<a class="headerlink" href="#intra-list-diversity" title="Permalink to this headline"></a></h3>
<p>Intra-List Diversity&#64;k measures the intra-list diversity of the recommendations when only k recommendations are
made to the user. It measures how the items in the item list for each user are different from each other. This metric has a
range in <span class="math notranslate nohighlight">\([0, 1]\)</span>. The higher this metric is, the more diversified items are recommended to
users. Let <span class="math notranslate nohighlight">\(U\)</span> denote the set of <span class="math notranslate nohighlight">\(N\)</span> unique users, <span class="math notranslate nohighlight">\(u_i\)</span> denotes the i-th user in the user set,
<span class="math notranslate nohighlight">\(v_p^{u_i}\)</span> are the item features of the p-th recommended item for user <span class="math notranslate nohighlight">\(u_i\)</span> and p &lt; q.</p>
<div class="math notranslate nohighlight">
\[Intra \mbox{-} list~diversity = 1 - \frac{1}{U}\sum_{i=1}^U average(consine\_similarity(v_p^{u_i}, v_q^{u_i}))\]</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="about_fairness.html" class="btn btn-neutral float-left" title="About Algorithmic Fairness" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="api.html" class="btn btn-neutral float-right" title="Jurity Public API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, FMR LLC.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>