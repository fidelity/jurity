

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>About Algorithmic Fairness &mdash; Jurity 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="#" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Jurity Public API" href="api.html" />
    <link rel="prev" title="Installation" href="install.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Jurity
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">About Algorithmic Fairness</a></li>
<li class="toctree-l1"><a class="reference internal" href="#about-recommenders-metrics">About Recommenders Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#binary-recommender-metrics">Binary Recommender Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ctr-click-through-rate">CTR: Click-through Rate</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ranking-recommender-metrics">Ranking Recommender Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#precision">Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recall">Recall</a></li>
<li class="toctree-l3"><a class="reference internal" href="#map-mean-average-precision">MAP: Mean Average Precision</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ndcg-normalized-discounted-cumulative-gain">NDCG: Normalized Discounted Cumulative Gain</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">Jurity Public API</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Jurity</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>About Algorithmic Fairness</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/about.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="about-algorithmic-fairness">
<span id="about"></span><h1>About Algorithmic Fairness<a class="headerlink" href="#about-algorithmic-fairness" title="Permalink to this headline">¶</a></h1>
<p>Below, see the mathematical definition for each of the fairness metrics in the library.</p>
<ul class="simple">
<li><p>Average odds denotes the average of difference in FPR and TPR for group 1 and group 2.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{1}{2} [(FPR_{D = \text{group 1}} - FPR_{D =
\text{group 2}}) + (TPR_{D = \text{group 2}} - TPR_{D
= \text{group 1}}))]\]</div>
<ul class="simple">
<li><p>Disparate Impact is the ratio of predictions for a “positive” outcome in a binary classification task between members of group 1 and group 2, respectively.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{P(\hat{Y} = 1 | D = \text{group 1})}
    {P(\hat{Y} = 1 | D = \text{group 2})}\]</div>
<ul class="simple">
<li><p>Equal Opportunity calculates the ratio of true positives to positive examples in the dataset, <span class="math notranslate nohighlight">\(TPR = TP/P\)</span>, conditioned on a protected attribute.</p></li>
<li><p>FNR Difference measures the equality (or lack thereof) of the false negative rates across groups. In practice, this metric is implemented as a difference between the metric value for group 1 and group 2.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[E[d(X)=0 \mid Y=1, g(X)] = E[d(X)=0, Y=1]\]</div>
<ul class="simple">
<li><p>Generalized entropy index is proposed as a unified individual and group fairness measure in <a class="footnote-reference brackets" href="#id2" id="id1">3</a>. With <span class="math notranslate nohighlight">\(b_i = \hat{y}_i - y_i + 1\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{E}(\alpha) = \begin{cases}
   \frac{1}{n \alpha (\alpha-1)}\sum_{i=1}^n\left[\left(\frac{b_i}{\mu}\right)^\alpha - 1\right] &amp;
   \alpha \ne 0, 1, \\
   \frac{1}{n}\sum_{i=1}^n\frac{b_{i}}{\mu}\ln\frac{b_{i}}{\mu} &amp; \alpha=1, \\
 -\frac{1}{n}\sum_{i=1}^n\ln\frac{b_{i}}{\mu},&amp; \alpha=0.
 \end{cases}\end{split}\]</div>
<dl>
<dt>References:</dt><dd><dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">3</a></span></dt>
<dd><p>T. Speicher, H. Heidari, N. Grgic-Hlaca, K. P. Gummadi, A. Singla, A. Weller, and M. B. Zafar,
A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual and Group Unfairness via
Inequality Indices, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2018.</p>
</dd>
</dl>
</dd>
</dl>
<ul class="simple">
<li><p>Predictive Equality is defined as the situation when accuracy of decisions is equal across two groups, as measured by false positive rate (FPR).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[E[d(X)|Y=0, g(X)] = E[d(X), Y=0]\]</div>
<ul class="simple">
<li><p>Statistical Parity measures the difference in probabilities of a positive outcome across two groups.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(\hat{Y} = 1 | D = \text{group 1}) - P(\hat{Y} = 1 | D = \text{group 2})\]</div>
<ul class="simple">
<li><p>Theil Index is the generalized entropy index with <span class="math notranslate nohighlight">\(\alpha = 1\)</span>. See Generalized Entropy index.</p></li>
</ul>
</div>
<div class="section" id="about-recommenders-metrics">
<h1>About Recommenders Metrics<a class="headerlink" href="#about-recommenders-metrics" title="Permalink to this headline">¶</a></h1>
<p>Jurity offers various standardized metrics for measuring the recommendation performance.
While the recommendation systems community agrees on these metrics, the implementations can be different, especially when it comes to edge-cases.</p>
<p>For the definitions below, <span class="math notranslate nohighlight">\(A\)</span> is the set of actual ratings for users and <span class="math notranslate nohighlight">\(P\)</span> is the set of predictions / recommendations.
Each <span class="math notranslate nohighlight">\(A_i\)</span> and <span class="math notranslate nohighlight">\(P_i\)</span> represents the list of actual items and list of recommended items, respectively, for a user <span class="math notranslate nohighlight">\(i\)</span>.</p>
<div class="section" id="binary-recommender-metrics">
<h2>Binary Recommender Metrics<a class="headerlink" href="#binary-recommender-metrics" title="Permalink to this headline">¶</a></h2>
<p>Binary recommender metrics directly measure the click interaction.</p>
<div class="section" id="ctr-click-through-rate">
<h3>CTR: Click-through Rate<a class="headerlink" href="#ctr-click-through-rate" title="Permalink to this headline">¶</a></h3>
<p>CTR measures the accuracy of the recommendations over the subset of user-item pairs that appear in both actual ratings and recommendations.</p>
<p>Let <span class="math notranslate nohighlight">\(M\)</span> denote the set of user-item pairs that appear in both actual ratings and recommendations, and <span class="math notranslate nohighlight">\(C(M_i)\)</span> be an indicator function that procudes <span class="math notranslate nohighlight">\(1\)</span> if the user clicked on the item, and <span class="math notranslate nohighlight">\(0\)</span> if they didn’t.</p>
<div class="math notranslate nohighlight">
\[CTR = \frac{1}{\left | M \right |}\sum_{i=1}^{\left | M \right |} C(M_i)\]</div>
</div>
</div>
<div class="section" id="ranking-recommender-metrics">
<h2>Ranking Recommender Metrics<a class="headerlink" href="#ranking-recommender-metrics" title="Permalink to this headline">¶</a></h2>
<p>Ranking metrics reward putting the clicked items on a higher position/rank than the others.
Some metrics use the relevance function <span class="math notranslate nohighlight">\(rel(P_{i,n})\)</span>, which is an indicator function that produces <span class="math notranslate nohighlight">\(1\)</span> if the predicted item at position <span class="math notranslate nohighlight">\(n\)</span> for user <span class="math notranslate nohighlight">\(i\)</span> is in the user’s relevant set of items.</p>
<p>All the ranking metrics operate on a filtered set of users such that only the users with relevant/clicked items are taken into account.
This is in line with industry practices.
There is a further filtering for precision related metrics (Precision&#64;k and MAP&#64;k) where each user also has to have a recommendation.
This is done to avoid divide by 0 errors.</p>
<div class="section" id="precision">
<h3>Precision<a class="headerlink" href="#precision" title="Permalink to this headline">¶</a></h3>
<p>Precision&#64;k measures how consistently a model is able to pinpoint the items a user would interact with.
A recommender system that only provides recommendations for 5% of the users will have a high precision if the users receiving the recommendations always interact with them.</p>
<div class="math notranslate nohighlight">
\[Precision&#64;k = \frac{1}{\left | A \cap P \right |}\sum_{i=1}^{\left | A \cap P \right |} \frac{\left | A_i \cap P_i[1:k] \right |}{\left | P_i[1:k] \right |}\]</div>
</div>
<div class="section" id="recall">
<h3>Recall<a class="headerlink" href="#recall" title="Permalink to this headline">¶</a></h3>
<p>Recall&#64;k measures whether a model can capture all the items the user has interacted with.
If __k__ is high enough, a recommender system can get a high recall even if it has a large amount of irrelevant recommendations, if it has also identified the relevant recommendations.</p>
<div class="math notranslate nohighlight">
\[Recall&#64;k = \frac{1}{\left | A \right |}\sum_{i=1}^{\left | A \right |} \frac{\left | A_i \cap P_i[1:k] \right |}{\left | A_i \right |}\]</div>
</div>
<div class="section" id="map-mean-average-precision">
<h3>MAP: Mean Average Precision<a class="headerlink" href="#map-mean-average-precision" title="Permalink to this headline">¶</a></h3>
<p>MAP&#64;k measures a position-sensitive version of Precision&#64;k, where getting the top-most recommendations more precise has a more important effect than getting the last recommendations correct.
When <span class="math notranslate nohighlight">\(k=1\)</span>, Precision&#64;k and MAP&#64;k are the same.</p>
<div class="math notranslate nohighlight">
\[MAP&#64;k = \frac{1}{\left | A \right |} \sum_{i=1}^{\left | A \right |} \frac{1}{min(k,\left | A_i \right |))}\sum_{n=1}^k Precision_i(n) \times rel(P_{i,n})\]</div>
</div>
<div class="section" id="ndcg-normalized-discounted-cumulative-gain">
<h3>NDCG: Normalized Discounted Cumulative Gain<a class="headerlink" href="#ndcg-normalized-discounted-cumulative-gain" title="Permalink to this headline">¶</a></h3>
<p>NDCG&#64;k measures the relevance of the ranked recommendations discounted by the rank at which they appear.
It is normalized to be between 0 and 1.
Improving the highest-ranked recommendations has a more important effect than improving the lowest-ranked recommendations.</p>
<div class="math notranslate nohighlight">
\[NDCG&#64;k = \frac{1}{\left | A \right |} \sum_{i=1}^{\left | A \right |} \frac {\sum_{r=1}^{\left | P_i \right |} \frac{rel(P_{i,r})}{log_2(r+1)}}{\sum_{r=1}^{\left | A_i \right |} \frac{1}{log_2(r+1)}}\]</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="api.html" class="btn btn-neutral float-right" title="Jurity Public API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="install.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, FMR LLC.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>